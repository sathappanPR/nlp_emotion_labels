{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"emotion-labels-train.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game â¤ï¸â¤ï...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Today I reached 1000 subscribers on YT!! , #go...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Singaholic121 Good morning, love! Happy first...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#BridgetJonesBaby is the best thing I've seen ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Just got back from seeing @GaryDelaney in Burs...   joy\n",
       "1  Oh dear an evening of absolute hilarity I don'...   joy\n",
       "2  Been waiting all week for this game â¤ï¸â¤ï...   joy\n",
       "3  @gardiner_love : Thank you so much, Gloria! Yo...   joy\n",
       "4  I feel so blessed to work with the family that...   joy\n",
       "5  Today I reached 1000 subscribers on YT!! , #go...   joy\n",
       "6  @Singaholic121 Good morning, love! Happy first...   joy\n",
       "7  #BridgetJonesBaby is the best thing I've seen ...   joy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fear       1147\n",
       "anger       857\n",
       "joy         823\n",
       "sadness     786\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dic = { 'fear'    :   3,\n",
    "                 'anger'   :   2,\n",
    "                 'joy'     :   1,\n",
    "                 'sadness' :   0\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"] = data[\"label\"].map(df_train_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= data.iloc[:,0:1]\n",
    "y_train = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game â¤ï¸â¤ï...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Just got back from seeing @GaryDelaney in Burs...\n",
       "1  Oh dear an evening of absolute hilarity I don'...\n",
       "2  Been waiting all week for this game â¤ï¸â¤ï...\n",
       "3  @gardiner_love : Thank you so much, Gloria! Yo...\n",
       "4  I feel so blessed to work with the family that..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just got back from seeing  garydelaney in burs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh dear an evening of absolute hilarity i don ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>been waiting all week for this game           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gardiner love   thank you so much  gloria  yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel so blessed to work with the family that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>vivienlloyd thank you so much  just home   st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>just put the winter duvet on                  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>silkinside  tommyjoeratliff that s so pretty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>bluesfestbyron second artist announcement loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>i can literally eat creamy pesto pasta topped ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     just got back from seeing  garydelaney in burs...\n",
       "1     oh dear an evening of absolute hilarity i don ...\n",
       "2     been waiting all week for this game           ...\n",
       "3      gardiner love   thank you so much  gloria  yo...\n",
       "4     i feel so blessed to work with the family that...\n",
       "...                                                 ...\n",
       "3608   vivienlloyd thank you so much  just home   st...\n",
       "3609  just put the winter duvet on                  ...\n",
       "3610   silkinside  tommyjoeratliff that s so pretty ...\n",
       "3611   bluesfestbyron second artist announcement loo...\n",
       "3612  i can literally eat creamy pesto pasta topped ...\n",
       "\n",
       "[3613 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertng headlines to lower case\n",
    "for index in X_train:\n",
    "    X_train[index]=X_train[index].str.lower()\n",
    "x_lower = X_train\n",
    "x_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_v = Vectorizer.fit_transform(x_lower[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(Vectorizer,open(\"Vectorizer.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_c = cv.fit_transform(x_lower[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"emotion-labels-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You must be knowing #blithe means (adj.)  Happ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Old saying 'A #smile shared is one gained for ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bridget Jones' Baby was bloody hilarious 😅 #Br...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Elaminova sparkling water makes your life spa...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm tired of everybody telling me to chill out...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  You must be knowing #blithe means (adj.)  Happ...   joy\n",
       "1  Old saying 'A #smile shared is one gained for ...   joy\n",
       "2  Bridget Jones' Baby was bloody hilarious 😅 #Br...   joy\n",
       "3  @Elaminova sparkling water makes your life spa...   joy\n",
       "4  I'm tired of everybody telling me to chill out...   joy"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dic = { 'fear'     :   3,\n",
    "                 'anger'   :   2,\n",
    "                 'joy'     :   1,\n",
    "                 'sadness' :   0\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"label\"] = df_test[\"label\"].map(df_test_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df_test.iloc[:,0:1]\n",
    "y_test = df_test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you must be knowing  blithe means  adj    happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old saying  a  smile shared is one gained for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bridget jones  baby was bloody hilarious    br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elaminova sparkling water makes your life spa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i m tired of everybody telling me to chill out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>why does candice constantly pout  gbbo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>redbus in  unhappy with  redbus cc  when i ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>aceoperative    no pull him afew weeks ago  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>i m buying art supplies and i m debating how s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>sainsburys could you ask your chafford hundre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0     you must be knowing  blithe means  adj    happ...\n",
       "1     old saying  a  smile shared is one gained for ...\n",
       "2     bridget jones  baby was bloody hilarious    br...\n",
       "3      elaminova sparkling water makes your life spa...\n",
       "4     i m tired of everybody telling me to chill out...\n",
       "...                                                 ...\n",
       "3137          why does candice constantly pout  gbbo   \n",
       "3138   redbus in  unhappy with  redbus cc  when i ta...\n",
       "3139   aceoperative    no pull him afew weeks ago  s...\n",
       "3140  i m buying art supplies and i m debating how s...\n",
       "3141   sainsburys could you ask your chafford hundre...\n",
       "\n",
       "[3142 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertng headlines to lower case\n",
    "for index in X_text:\n",
    "    X_text[index]=X_text[index].str.lower()\n",
    "X_text_lower = X_text\n",
    "X_text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_v = Vectorizer.transform(X_text_lower['text']) # Vectorizer\n",
    "test_dataset_c = cv.transform(X_text_lower['text']) # Countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = RandomForestClassifier()\n",
    "rdw = rd.fit(x_tran_v,y)\n",
    "predictions = rd.predict(test_dataset_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[490  18  44 121]\n",
      " [ 15 587  12 100]\n",
      " [ 35  22 554 149]\n",
      " [ 75  22  39 859]]\n",
      "\n",
      "0.792488860598345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76       673\n",
      "           1       0.90      0.82      0.86       714\n",
      "           2       0.85      0.73      0.79       760\n",
      "           3       0.70      0.86      0.77       995\n",
      "\n",
      "    accuracy                           0.79      3142\n",
      "   macro avg       0.81      0.79      0.80      3142\n",
      "weighted avg       0.80      0.79      0.79      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Import library to check accuracy\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "matrix=confusion_matrix(y_test,predictions)\n",
    "print(matrix)\n",
    "print()\n",
    "score=accuracy_score(y_test,predictions)\n",
    "print(score)\n",
    "\n",
    "report=classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences labels is joy \n",
    "fear    =>  3, \n",
    "anger   =>  2,\n",
    "joy    =>  1,\n",
    "sadness =>  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @theclobra lol I thought maybe, couldn't decide if there was levity or not                                  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch           => joy\n",
    "# @tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism                               => joy\n",
    "# I love my family so much #lucky #grateful #smartassfamily  #love                                            => joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.predict(Vectorizer.transform([\"@theclobra lol I thought maybe, couldn't decide if there was levity or not\",\"@tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism\",\"I love my family so much #lucky #grateful #smartassfamily  #love\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RandomForestClassifier Without any hyperparameter tuning, the data is overfiting on \"fear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [300, 442, 584, 726, 868, 1010, 1152, 1294, 1436, 1578, 1721, 1863, 2005, 2147, 2289, 2431, 2573, 2715, 2857, 3000], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [20, 55, 91, 126, 162, 197, 233, 268, 304, 339, 375, 410, 446, 481, 517, 552, 588, 623, 659, 694, 730, 765, 801, 836, 872, 907, 943, 978, 1014, 1050], 'min_samples_split': [3, 8, 9, 12, 20], 'min_samples_leaf': [3, 5, 6, 7, 10], 'criterion': ['entropy', 'gini']}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 300, stop = 3000, num = 20)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 1050,30)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [3, 8,9 ,12,20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [3, 5, 6,7,10]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "              'criterion':['entropy','gini']}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [20, 55, 91, 126, 162, 197,\n",
       "                                                      233, 268, 304, 339, 375,\n",
       "                                                      410, 446, 481, 517, 552,\n",
       "                                                      588, 623, 659, 694, 730,\n",
       "                                                      765, 801, 836, 872, 907,\n",
       "                                                      943, 978, 1014, 1050],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [3, 5, 6, 7, 10],\n",
       "                                        'min_samples_split': [3, 8, 9, 12, 20],\n",
       "                                        'n_estimators': [300, 442, 584, 726,\n",
       "                                                         868, 1010, 1152, 1294,\n",
       "                                                         1436, 1578, 1721, 1863,\n",
       "                                                         2005, 2147, 2289, 2431,\n",
       "                                                         2573, 2715, 2857,\n",
       "                                                         3000]},\n",
       "                   random_state=100, verbose=2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_randomcv=RandomizedSearchCV(estimator=rd,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "### fit the randomized model\n",
    "rf_randomcv.fit(x_train_v,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[495  20  42 116]\n",
      " [ 13 595  18  88]\n",
      " [ 38  21 557 144]\n",
      " [ 68  29  42 856]]\n",
      "\n",
      "0.7966263526416295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77       673\n",
      "           1       0.89      0.83      0.86       714\n",
      "           2       0.85      0.73      0.79       760\n",
      "           3       0.71      0.86      0.78       995\n",
      "\n",
      "    accuracy                           0.80      3142\n",
      "   macro avg       0.81      0.79      0.80      3142\n",
      "weighted avg       0.81      0.80      0.80      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=rf_randomcv.predict(test_dataset_v)\n",
    "matrix=confusion_matrix(y_test,predictions)\n",
    "print(matrix)\n",
    "print()\n",
    "score=accuracy_score(y_test,predictions)\n",
    "print(score)\n",
    "report=classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences labels is joy \n",
    "fear    =>  3, \n",
    "anger   =>  2,\n",
    "joy    =>   1,\n",
    "sadness =>  0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @theclobra lol I thought maybe, couldn't decide if there was levity or not                                  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch           => joy\n",
    "# @tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism                               => joy\n",
    "# I love my family so much #lucky #grateful #smartassfamily  #love                                            => joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"@theclobra lol I thought maybe, couldn't decide if there was levity or not\",\"@tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism\",\"I love my family so much #lucky #grateful #smartassfamily  #love\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot #terror                 =>    fear\n",
    "# On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot                         =>    fear\n",
    "# âŠ° @FrameOfAnAngel âŠ± \\n\\n+ Of them. I'm here for answers, and if I scare her to death, there won't be answers for me. \\n\\nSo instead, I just + =>    fear\n",
    "# But I was so intrigued by your style, boy.Always been a sucker for a wild boy #alarm -@AnneMarieIAm                                               =>    fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot #terror\",\"But I was so intrigued by your style, boy.Always been a sucker for a wild boy #alarm -@AnneMarieIAm \",\" âŠ° @FrameOfAnAngel âŠ± \\n\\n+ Of them. I'm here for answers, and if I scare her to death, there won't be answers for me. \\n\\nSo instead, I just +\",\"On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @DailyMirror i love how theres no outrage that it's a white man but if it was a black man them BLM would be all over it regardless of reason =>  anger\n",
    "# Me being on my dean really saving a lot of ppl, bc I don't snap nomore &amp; it take so much out of me.                                      =>  anger\n",
    "# @TrueAggieFan oh so that's where Brian was! Where was my invite? #offended                                                                   =>  anger\n",
    "# Sorry guys I have absolutely no idea what time i'll be on cam tomorrow but will keep you posted.                                             =>  anger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"@DailyMirror i love how theres no outrage that it's a white man but if it was a black man them BLM would be all over it regardless of reason\",\"Me being on my dean really saving a lot of ppl, bc I don't snap nomore &amp; it take so much out of me.\",\"@TrueAggieFan oh so that's where Brian was! Where was my invite? #offended\",\"Sorry guys I have absolutely no idea what time i'll be on cam tomorrow but will keep you posted.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @trashcami this cured my depression                                                                                                            =>    sadness\n",
    "# This world has some serious issues we should all go to therapy                                                                                 =>    sadness\n",
    "# If I had a little bit of extra money I would blow the whole paycheck and go to one of the two of @KygoMusic's concerts in LA. #serious         =>    sadness\n",
    "# @kempicepoland don't think he did, and he didn't have the rucksack or laptop in his possession, murky business, on that note I'm away to bed   =>    sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"@trashcami this cured my depression\",\"@kempicepoland don't think he did, and he didn't have the rucksack or laptop in his possession, murky business, on that note I'm away to bed\",\"If I had a little bit of extra money I would blow the whole paycheck and go to one of the two of @KygoMusic's concerts in LA. #serious\",\"This world has some serious issues we should all go to therapy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"This world has some serious issues we should all go to therapy\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\",\"Me being on my dean really saving a lot of ppl, bc I don't snap nomore &amp; it take so much out of me.\",\"But I was so intrigued by your style, boy.Always been a sucker for a wild boy #alarm -@AnneMarieIAm \"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_grid.predict(cv.transform([\"i am happy\",\"i am happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fiel = \"npl_model.pkl\"\n",
    "pickle.dump(rf_randomcv,open(fiel,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_hyper_tuneing.predict(cv.transform([\"On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot #terror\",\"But I was so intrigued by your style, boy.Always been a sucker for a wild boy #alarm -@AnneMarieIAm \",\" âŠ° @FrameOfAnAngel âŠ± \\n\\n+ Of them. I'm here for answers, and if I scare her to death, there won't be answers for me. \\n\\nSo instead, I just +\",\"On @Varneyco/@FoxBusiness to talk latest on #Chelsea Bombing + #Ahmad_Khan_Rahami's trips to #Afghanistan/#Pakistan #tcot\",]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train_v,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[495  20  42 116]\n",
      " [ 13 595  18  88]\n",
      " [ 38  21 557 144]\n",
      " [ 68  29  42 856]]\n",
      "\n",
      "0.7966263526416295\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77       673\n",
      "           1       0.89      0.83      0.86       714\n",
      "           2       0.85      0.73      0.79       760\n",
      "           3       0.71      0.86      0.78       995\n",
      "\n",
      "    accuracy                           0.80      3142\n",
      "   macro avg       0.81      0.79      0.80      3142\n",
      "weighted avg       0.81      0.80      0.80      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=dt.predict(test_dataset_v)\n",
    "matrix=confusion_matrix(y_test,predictions)\n",
    "print(matrix)\n",
    "print()\n",
    "score=accuracy_score(y_test,predictions)\n",
    "print(score)\n",
    "report=classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @theclobra lol I thought maybe, couldn't decide if there was levity or not                                  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch  => joy\n",
    "# Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch           => joy\n",
    "# @tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism                               => joy\n",
    "# I love my family so much #lucky #grateful #smartassfamily  #love                                            => joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(cv.transform([\"@theclobra lol I thought maybe, couldn't decide if there was levity or not\",\"@tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism\",\"I love my family so much #lucky #grateful #smartassfamily  #love\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier without any  Hyperparameter Tuning, the dataset is overfiting on \"fear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tunin DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 27, 28, 28, 28, 29, 29, 29, 30, 30, 30, 31, 31, 31, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35, 36, 36, 36, 37, 37, 37, 38, 38, 38, 39, 39, 40], 'min_samples_split': [4, 8, 12, 16, 21], 'min_samples_leaf': [2, 4, 6, 8, 10], 'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}\n"
     ]
    }
   ],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "splitter = [\"best\", \"random\"]\n",
    "max_depth = [int(x) for x in np.linspace(20, 40,60)]\n",
    "min_samples_split = [4,8,12,16,21]\n",
    "min_samples_leaf = [2,4,6,8,10]\n",
    "min_weight_fraction_leaf = [.2,.4,.6,.8,.9]\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "Decision_grid = {'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'criterion': criterion,\n",
    "                'splitter' : splitter}\n",
    "print(Decision_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=DecisionTreeClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [20, 20, 20, 21, 21, 21,\n",
       "                                                      22, 22, 22, 23, 23, 23,\n",
       "                                                      24, 24, 24, 25, 25, 25,\n",
       "                                                      26, 26, 26, 27, 27, 27,\n",
       "                                                      28, 28, 28, 29, 29, 29, ...],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [2, 4, 6, 8, 10],\n",
       "                                        'min_samples_split': [4, 8, 12, 16, 21],\n",
       "                                        'splitter': ['best', 'random']},\n",
       "                   random_state=420, verbose=2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_tune=RandomizedSearchCV(estimator=dt,param_distributions=Decision_grid,n_iter=100,cv=3,verbose=2,n_jobs=-1,random_state = 420)\n",
    "\n",
    "Decision_tune.fit(traindataset,data_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'splitter': 'random',\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 38,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_tune.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision_tuneing=Decision_tune.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  47    7    0  732]\n",
      " [   0  147    0  676]\n",
      " [   5   11   48  793]\n",
      " [   2   10    0 1135]]\n",
      "Accuracy Score 0.3811237199003598\n",
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.06      0.11       786\n",
      "           1       0.84      0.18      0.29       823\n",
      "           2       1.00      0.06      0.11       857\n",
      "           3       0.34      0.99      0.51      1147\n",
      "\n",
      "    accuracy                           0.38      3613\n",
      "   macro avg       0.76      0.32      0.25      3613\n",
      "weighted avg       0.73      0.38      0.28      3613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=Decision_tuneing.predict(test_dataset)\n",
    "print(confusion_matrix(data_test_y,y_pred))\n",
    "print(\"Accuracy Score {}\".format(accuracy_score(data_test_y,y_pred)))\n",
    "print(\"Classification report: {}\".format(classification_report(data_test_y,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decision_tuneing.predict(cv.transform([\"@theclobra lol I thought maybe, couldn't decide if there was levity or not\",\"@tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism\",\"I love my family so much #lucky #grateful #smartassfamily  #love\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsvm = svm.SVC(kernel = \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsvm.fit(x_tran_v,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsvm.predict(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelsvm.predict(Vectorizer.transform([\"@theclobra lol I thought maybe, couldn't decide if there was levity or not\",\"@tomderivan73 ðŸ˜...I'll just people watch and enjoy a rare show of optimism\",\"I love my family so much #lucky #grateful #smartassfamily  #love\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day.  #challenge #kashmir #baloch\",\"Nawaz Sharif is getting more funnier than @kapilsharmak9 day by day. #laughter #challenge #kashmir #baloch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=v.predict(test_dataset)\n",
    "print(confusion_matrix(data_test_y,y_pred))\n",
    "print(\"Accuracy Score {}\".format(accuracy_score(data_test_y,y_pred)))\n",
    "print(\"Classification report: {}\".format(classification_report(data_test_y,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
